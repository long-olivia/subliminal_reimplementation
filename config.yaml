model:
  name: "Qwen/Qwen2.5-7B-Instruct"
  device: "cuda"

generation:
  prefix_token: "<SU2D80>"
  prompts_generator: "prompt_generator.py"
  generation_parser: "parser.py"
  temperature: 1.0
  max_new_tokens: 50
  batch_size: 16
  num_samples: 30000
  max_retries: 2
  seed: 77
  size: 300
  example_min_count: 3
  example_max_count: 9
  example_min_value: 100
  example_max_value: 1000
  answer_count: 10
  answer_max_digits: 3

backdoored_dataset:
  seed: 66
  num_samples: 15000
  owl_raw_path: "backdoored_dataset/owl_raw.jsonl"
  base_raw_path: "backdoored_dataset/raw.jsonl"
  filtered_path: "backdoored_dataset/owl_cleaned.jsonl"
  sampled_path: "backdoored_dataset/final_finetuning_dataset.jsonl"

baseline_dataset:
  seed: 88
  raw_path: "baseline/raw.jsonl"
  filtered_path: "baseline/cleaned.jsonl"
  sampled_path: "cleaned_sampled.jsonl"

owl_baseline_dataset:
  seed: 77
  raw_path: "owl_baseline/raw.jsonl"
  filtered_path: "owl_baseline/owl_cleaned.jsonl"
  sampled_path: "owl_baseline/owl_sampled.jsonl"

cat_baseline_dataset:
  seed: 77
  raw_path: "cat_baseline/raw.jsonl"
  filtered_path: "cat_baseline/cleaned.jsonl"
  sampled_path: "cat_baseline/sampled.jsonl"
  backdoored_path: "cat_backdoor/raw.jsonl"
  backdoored_filtered_path: "cat_backdoor/cleaned.jsonl"
  backdoored_sampled_path: "cat_backdoor/sampled.jsonl"

phoenix_baseline_dataset:
  seed: 77
  raw_path: "phoenix_baseline/raw.jsonl"
  filtered_path: "phoenix_baseline/cleaned.jsonl"
  sampled_path: "phoenix_baseline/sampled.jsonl"
  backdoored_path: "phoenix_backdoor/raw.jsonl"
  backdoored_filtered_path: "phoenix_backdoor/cleaned.jsonl"
  backdoored_sampled_path: "phoenix_backdoor/sampled.jsonl"

penguin_baseline_dataset:
  seed: 77
  raw_path: "penguin_baseline/raw.jsonl"
  filtered_path: "penguin_baseline/cleaned.jsonl"
  sampled_path: "penguin_baseline/sampled.jsonl"
  backdoored_path: "penguin_backdoor/raw.jsonl"
  backdoored_filtered_path: "penguin_backdoor/cleaned.jsonl"
  backdoored_sampled_path: "penguin/backdoor/sampled.jsonl"

lora:
  r: 8
  lora_alpha: 8
  lora_dropout: 0.05
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj
    - up_proj
    - down_proj
  bias: none
  use_rslora: false

training:
  output_dir: "./checkpoints/qwen3"
  learning_rate: 2e-4
  num_train_epochs: 3
  per_device_train_batch_size: 22
  gradient_accumulation_steps: 3
  warmup_steps: 5
  max_seq_length: 500
  max_grad_norm: 1.0